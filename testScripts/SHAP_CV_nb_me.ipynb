{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing, linear_model\n",
    "import re\n",
    "\n",
    "sys.path.append('../LSP_Repo/')\n",
    "\n",
    "import classifyFunctions\n",
    "import helperFunctions as hf\n",
    "import initFunctions as init\n",
    "\n",
    "# Load my data\n",
    "# Load Pickle\n",
    "lightsheet_data = pd.read_pickle('lightsheet_data.pkl')\n",
    "\n",
    "classifyDict = dict()\n",
    "classifyDict['featureSel_scale'] = False # True, False\n",
    "classifyDict['featureSel_linkage'] = 'average'  # 'average', 'complete', 'single'\n",
    "classifyDict['featureSel_distance'] = 'correlation' # 'correlation, 'cosine', 'euclidean'\n",
    "classifyDict['featureSel_filter'] = True\n",
    "classifyDict['featureSel_agg'] = True\n",
    "classifyDict['cluster_count'] = 10\n",
    "\n",
    "classifyDict['data'] = 'cell_density' #cell_density, count\n",
    "classifyDict['feature'] = 'abbreviation'\n",
    "classifyDict['label'] = 'drug'\n",
    "\n",
    "# classifyDict['model'] = 'LogRegElastic'\n",
    "classifyDict['model'] = 'LogRegL2'\n",
    "\n",
    "classifyDict['model_featureSel'] = 'L1'\n",
    "classifyDict['model_classify'] = 'L2'\n",
    "\n",
    "classifyDict['shuffle'] = True\n",
    "classifyDict['includeSAL'] = True\n",
    "classifyDict['include6FDET'] = True\n",
    "classifyDict['remove_high_corr'] = True\n",
    "classifyDict['corrThreshold'] = 0.9\n",
    "classifyDict['gridCV'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Paths to data and output\n",
    "dirDict = dict()\n",
    "rootDir = 'C:\\OneDrive\\MEng\\KwanLab\\cFosProject2\\\\'\n",
    "dirDict['atlasDir'] = rootDir + 'Atlas\\\\'\n",
    "dirDict['dataDir'] = rootDir + 'Data\\\\'\n",
    "dirDict['B1'] =       dirDict['dataDir'] + 'lightSheetV1\\\\'\n",
    "dirDict['B2'] =       dirDict['dataDir'] + 'lightSheetV2Rev\\\\'   #3/6/23 - Looking at the new, Realigned batch 2 data. #Realigned\n",
    "dirDict['B2_Orig'] =  dirDict['dataDir'] + 'lightSheetV2\\\\'\n",
    "dirDict['B3'] =       dirDict['dataDir'] + 'lightSheetV3\\\\'      #3/6/23 - Batch 3 with MDMA\n",
    "\n",
    "batchSplit = False          # Splits drugs from the first batch of data, from the second, from the 3rd. Batch 1 is labeled with 'a' (aSAL, aKET, aPSI), Batch 3 (cKET, MDMA)\n",
    "splitTag  = ['a', '', 'c']  # Appended the to beginning of data from the first batch (PSI, KET, SAL -> aPSI, KET, aSAL).\n",
    "testSplit = False           # Splits an individual drug for the sake of examining self-similarity\n",
    "oldBatch2 = False\n",
    "\n",
    "debugOutputs = False        # Saves csvs at intervals\n",
    "scalingFactor = True        # Applies 1/total_cells as a scaling factor per mouse.\n",
    "debug_ROI = ['Dorsal nucleus raphe']\n",
    "outputFormat = 'png'\n",
    "\n",
    "switchDir = dict(testSplit=testSplit, batchSplit=batchSplit, splitTag=splitTag, oldBatch2=oldBatch2, debugOutputs=debugOutputs, scalingFactor=scalingFactor, debug_ROI=debug_ROI, outputFormat=outputFormat)\n",
    "\n",
    "# Make directories, and add their strings to the directory dictionary.\n",
    "dirDict = init.createDirs(rootDir, switchDir, dirDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See if I need to binarize, then try again.\n",
    "myData = True\n",
    "\n",
    "# Load data\n",
    "if not myData:\n",
    "    url = 'https://raw.githubusercontent.com/Sketchjar/MachineLearningHD/main/boston_data.csv'\n",
    "    df = pd.read_csv(url); df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "    X, y = df.drop('Target', axis=1), df.Target\n",
    "else:\n",
    "    \n",
    "    if classifyDict['featureSel_filter']:\n",
    "        pandasdf = hf.filter_and_agg_Data(lightsheet_data, classifyDict)\n",
    "\n",
    "    if classifyDict['featureSel_agg']:\n",
    "        ls_data_agg = hf.agg_cluster(pandasdf, classifyDict, dirDict)\n",
    "    else:\n",
    "        ls_data_agg = pandasdf.pivot(index='dataset', columns=classifyDict['feature'], values=classifyDict['data'])\n",
    "\n",
    "    df = ls_data_agg.sample(frac=1)\n",
    "\n",
    "    # X, y, featureNames, numYDict = hf.reformatData(pandasdf, classifyDict)\n",
    "\n",
    "    # y = np.array([x[0:-1] for x in np.array(ls_data_agg.index)])\n",
    "    yStr = np.array([x[0:-1] for x in np.array(df.index)])\n",
    "    yDict = dict(zip(np.unique(yStr), range(1, len(np.unique(yStr))+1)))\n",
    "    y = np.array([yDict[x] for x in yStr])\n",
    "\n",
    "    X = df.reset_index(drop=True)\n",
    "    \n",
    "    featureNames = np.array(df.columns)\n",
    "    # numYDict = {y: i for i, y in enumerate(np.unique(y))}\n",
    "    numYDict = {value: key for key, value in yDict.items()}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish CV scheme\n",
    "CV = KFold(n_splits=8, shuffle=True, random_state=10)\n",
    "\n",
    "# Libraries for this section \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn import preprocessing\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y)\n",
    "yAll = lb.transform(y)\n",
    "CV_repeats = 100\n",
    "splitCount = 8\n",
    "\n",
    "for drug_idx in range(0,7):\n",
    "\n",
    "    drugName = lb.classes_[drug_idx]\n",
    "    print(f'Working on {drugName}')\n",
    "    y = pd.Series(yAll[:,drug_idx])\n",
    "\n",
    "    ix_training, ix_test = [], []\n",
    "    # Loop through each fold and append the training & test indices to the empty lists above\n",
    "    for fold in CV.split(df):\n",
    "        ix_training.append(fold[0]), ix_test.append(fold[1])\n",
    "\n",
    "    SHAP_values_per_fold = []\n",
    "\n",
    "    np.random.seed(1) # Reproducibility \n",
    "    # Make a list of random integers between 0 and 10000 of length = CV_repeats to act as different data splits\n",
    "    random_states = np.random.randint(10000, size=CV_repeats) \n",
    "\n",
    "    ######## Use a dict to track the SHAP values of each observation per CV repitition \n",
    "\n",
    "    shap_values_per_cv = dict()\n",
    "    for sample in X.index:\n",
    "        ## Create keys for each sample\n",
    "        shap_values_per_cv[sample] = {} \n",
    "        ## Then, keys for each CV fold within each sample\n",
    "        for CV_repeat in range(CV_repeats):\n",
    "            shap_values_per_cv[sample][CV_repeat] = {}\n",
    "\n",
    "    # Split data, establish model, fit model, make prediction, score model, print result\n",
    "    for i, CV_repeat in enumerate(range(CV_repeats)): #-#-#\n",
    "        #Verbose \n",
    "        # print('\\n------------ CV Repeat number:', CV_repeat)\n",
    "        #Establish CV scheme\n",
    "        CV = KFold(n_splits=splitCount, shuffle=True, random_state=random_states[i]) # Set random state \n",
    "\n",
    "        ix_training, ix_test = [], []\n",
    "        # Loop through each fold and append the training & test indices to the empty lists above\n",
    "        for fold in CV.split(df):\n",
    "            ix_training.append(fold[0]), ix_test.append(fold[1])\n",
    "            \n",
    "        ## Loop through each outer fold and extract SHAP values \n",
    "        for i, (train_outer_ix, test_outer_ix) in enumerate(zip(ix_training, ix_test)): \n",
    "            #Verbose\n",
    "            # print('\\n------ Fold Number:',i)\n",
    "            X_train, X_test = X.iloc[train_outer_ix, :], X.iloc[test_outer_ix, :]\n",
    "            y_train, y_test = y.iloc[train_outer_ix], y.iloc[test_outer_ix]\n",
    "\n",
    "            if not myData:\n",
    "                model = RandomForestRegressor(random_state=10) # Random state for reproducibility (same results every time)\n",
    "                fit = model.fit(X_train, y_train)\n",
    "                yhat = fit.predict(X_test)\n",
    "                result = mean_squared_error(y_test, yhat)\n",
    "\n",
    "                # Use SHAP to explain predictions\n",
    "                explainer = shap.Explainer(model)\n",
    "                shap_values = explainer.shap_values(X_train)\n",
    "            else:\n",
    "                model = sklearn.linear_model.LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000, dual=True)\n",
    "                pipeline = make_pipeline(preprocessing.RobustScaler(), model)\n",
    "                pipelineT = make_pipeline(preprocessing.RobustScaler())\n",
    "                pipelineT.fit(X_train, y_train)\n",
    "                \n",
    "                # Transform data while preserving pandas dataframe structure and index\n",
    "                X_train_trans = pd.DataFrame(pipelineT.transform(X_train), columns=X_train.columns)\n",
    "                X_train_trans.set_index(X_train.index)\n",
    "                # X_train_trans = X_train.copy()\n",
    "\n",
    "                fit = pipeline.fit(X_train, y_train)\n",
    "\n",
    "                explainer = shap.Explainer(pipeline._final_estimator, X_train_trans, feature_names=X.columns, max_iter=1000)\n",
    "                shap_values = explainer.shap_values(X_train_trans)\n",
    "                # shap.plots.beeswarm(shap_values)\n",
    "                \n",
    "            # Extract SHAP information per fold per sample \n",
    "            for i, test_index in enumerate(test_outer_ix):\n",
    "                shap_values_per_cv[test_index][CV_repeat] = shap_values[i] #-#-#\n",
    "\n",
    "    # Establish lists to keep average Shap values, their Stds, and their min and max\n",
    "    average_shap_values, stds, ranges = [],[],[]\n",
    "\n",
    "    for i in range(0,len(df)):\n",
    "        df_per_obs = pd.DataFrame.from_dict(shap_values_per_cv[i]) # Get all SHAP values for sample number i\n",
    "        # Get relevant statistics for every sample \n",
    "        average_shap_values.append(df_per_obs.mean(axis=1).values) \n",
    "        stds.append(df_per_obs.std(axis=1).values)\n",
    "        ranges.append(df_per_obs.max(axis=1).values-df_per_obs.min(axis=1).values)\n",
    "\n",
    "    new_index = [ix for ix_test_fold in ix_test for ix in ix_test_fold]\n",
    "    \n",
    "    shap.summary_plot(np.array(average_shap_values), X, show = False)\n",
    "    plt.title(f'{drugName} - Average SHAP values after {CV_repeats}x cross-validation')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cFosV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
