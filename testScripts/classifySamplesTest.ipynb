{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test script for classification\n",
    "# Import packages\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Good idea to add this folder to the json.settings file as \"python.analysis.extraPaths\".\n",
    "sys.path.append('../functionScripts/')\n",
    "\n",
    "import plotFunctions # totalCountsPlot, data_heatmap, correlation_plot\n",
    "import analysisFunctions\n",
    "import initFunctions as initf #import createDirs, debugReport, loadLightSheetData\n",
    "import classifyFunctions\n",
    "import helperFunctions\n",
    "\n",
    "importlib.reload(initf)\n",
    "\n",
    "# Set Paths to data and output\n",
    "dirDict = dict()\n",
    "rootDir = 'C:\\OneDrive\\KwanLab\\Lightsheet_cFos_Pipeline\\\\'\n",
    "dirDict['atlasDir'] = rootDir + 'Atlas\\\\'\n",
    "dirDict['dataDir'] = rootDir + 'Data\\\\'\n",
    "dirDict['B1'] =       dirDict['dataDir'] + 'lightSheetV1\\\\'\n",
    "dirDict['B2'] =       dirDict['dataDir'] + 'lightSheetV2Rev\\\\'   #3/6/23 - Looking at the new, Realigned batch 2 data. #Realigned\n",
    "dirDict['B2_Orig'] =  dirDict['dataDir'] + 'lightSheetV2\\\\'\n",
    "dirDict['B3'] =       dirDict['dataDir'] + 'lightSheetV3\\\\'      #3/6/23 - Batch 3 with MDMA\n",
    "\n",
    "batchSplit = False          # Splits drugs from the first batch of data, from the second, from the 3rd. Batch 1 is labeled with 'a' (aSAL, aKET, aPSI), Batch 3 (cKET, MDMA)\n",
    "splitTag  = ['a', '', 'c']  # Appended the to beginning of data from the first batch (PSI, KET, SAL -> aPSI, KET, aSAL).\n",
    "testSplit = False           # Splits an individual drug for the sake of examining self-similarity\n",
    "oldBatch2 = False\n",
    "\n",
    "debugOutputs = False        # Saves csvs at intervals\n",
    "scalingFactor = True        # Applies 1/total_cells as a scaling factor per mouse.\n",
    "debug_ROI = ['Dorsal nucleus raphe']\n",
    "outputFormat = 'png'\n",
    "\n",
    "switchDir = dict(testSplit=testSplit, batchSplit=batchSplit, splitTag=splitTag, oldBatch2=oldBatch2, debugOutputs=debugOutputs, scalingFactor=scalingFactor, debug_ROI=debug_ROI, outputFormat=outputFormat)\n",
    "\n",
    "# Make directories, and add their strings to the directory dictionary.\n",
    "dirDict = initf.createDirs(rootDir, switchDir, dirDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload in case anything updated in these functions\n",
    "importlib.reload(classifyFunctions)\n",
    "importlib.reload(plotFunctions)\n",
    "importlib.reload(helperFunctions)\n",
    "\n",
    "classifyDict = dict()\n",
    "\n",
    "# Parameters for pivoting the data\n",
    "classifyDict['data'] = 'count_norm' #cell_density, count, count_norm, density_norm\n",
    "classifyDict['feature'] = 'abbreviation'\n",
    "classifyDict['label'] = 'drug' #'drug', 'class_5HT2A', 'class_KetPsi', 'class_5HTR', 'class_Trypt', 'class_Speed', 'class_Psy_NMDA', 'class_SSRI' # Defined in 'helperFunctions.create_drugClass_dict'\n",
    "\n",
    "# Parameters for feature scaling and aggregation\n",
    "classifyDict['featurefilt'] = False\n",
    "classifyDict['featureAgg'] = False\n",
    "classifyDict['featureSel_linkage'] = 'average'  # 'average', 'complete', 'single', 'ward' (if euclidean)\n",
    "classifyDict['featureSel_distance'] = 'correlation' # 'correlation, 'cosine', 'euclidean'\n",
    "classifyDict['cluster_count'] = 100 # Number of clusters to generate. Not used at the moment.\n",
    "classifyDict['cluster_thres'] = 0.2 # Anything closer than this is merged into a cluster\n",
    " \n",
    "# Parameters for Preprocessing and feature selection\n",
    "classifyDict['model_featureTransform'] = True # True, False - applies the Yeo-Johnson power transformation to shift features into more gaussian distributions.\n",
    "classifyDict['model_featureScale'] = True # True, False\n",
    "classifyDict['model_featureSel'] = 'Boruta' # 'Univar', 'mutInfo', 'RFE', 'MRMR', 'Fdr', 'Fwe_BH', 'Fwe', 'Boruta', 'None'\n",
    "classifyDict['model_featureSel_alpha'] = 0.05 # Used for Fdr, Fwe, and Fwe_BH\n",
    "\n",
    "# If Fdr/Fwe/None are not used for feature selection, the number of k feature must be preset\n",
    "classifyDict['model_featureSel_mode'] = 'modelPer' # 'gridCV', 'modelPer'\n",
    "# classifyDict['model_featureSel_k'] = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "classifyDict['model_featureSel_k'] = [30]\n",
    "\n",
    "# Parameters for classification\n",
    "classifyDict['model'] = 'LogRegL2' #'LogRegL2', 'LogRegL1', 'ElasticNet', 'LDA', 'RandomForest'\n",
    "classifyDict['multiclass'] = 'multinomial' # 'ovr', 'multinomial'\n",
    "classifyDict['max_iter'] = 100\n",
    "classifyDict['CVstrat'] = 'ShuffleSplit' #'StratKFold', 'ShuffleSplit'\n",
    "\n",
    "# ParamGrid Features - in instances where gridCV is set to true, these are the parameters that will be tested.\n",
    "paramGrid = dict()\n",
    "paramGrid['classif__l1_ratio'] = [0, 0.1, 0.25, 0.5, 0.75, 0.9, 1]          # used for ElasticNet\n",
    "# paramGrid['classif__C'] = [0.001, 0.01, 0.1, 1, 10]                    # used for LogisticRegression\n",
    "paramGrid['classif__C'] = [1]                    # used for LogisticRegression\n",
    "classifyDict['pGrid'] = paramGrid\n",
    "\n",
    "classifyDict['shuffle'] = False\n",
    "classifyDict['gridCV'] = False\n",
    "\n",
    "if classifyDict['CVstrat'] == 'ShuffleSplit':\n",
    "    classifyDict['CV_count'] = 100 # Number of folds for cross-validation\n",
    "else:\n",
    "    # K fold stratified can only afford n_classes of folds\n",
    "    classifyDict['CV_count'] = 8 \n",
    "\n",
    "if classifyDict['label'] == 'drug':\n",
    "    classifyDict['test_size'] = 1/8\n",
    "    classifyDict['innerFold'] = 7\n",
    "else:\n",
    "    classifyDict['test_size'] = 1/4\n",
    "    classifyDict['innerFold'] = 4\n",
    "\n",
    "classifyDict['saveLoadswitch'] = True\n",
    "dirDict['uniqID'] = 'try100'\n",
    "\n",
    "# Load Pickle\n",
    "lightsheet_data = pd.read_pickle('lightsheet_data.pkl')\n",
    "\n",
    "# Add a normalized version of counts\n",
    "lightsheet_data.loc[:, 'count_norm'] = lightsheet_data.loc[:, 'count']/lightsheet_data.loc[:, 'total_cells']\n",
    "lightsheet_data.loc[:, 'count_norm'] = lightsheet_data.loc[:, 'count_norm'] * int(np.floor(lightsheet_data.loc[:, 'total_cells'].mean()))\n",
    "lightsheet_data.loc[:, 'density_norm'] = lightsheet_data.loc[:, 'count_norm']/lightsheet_data.loc[:, 'volume_(mm^3)']\n",
    "\n",
    "lightsheet_data = lightsheet_data.fillna(0)\n",
    "\n",
    "# Big Data - the dataset prior to filtering based on summary structures\n",
    "# lightsheet_data = pd.read_pickle('lightsheet_data_big.pkl')\n",
    "# lightsheet_data.rename(columns={'Brain Area': 'Brain_Area'}, inplace=True)\n",
    "\n",
    "# lightsheet_data_filt = helperFunctions.filter_features(lightsheet_data, classifyDict)\n",
    "# plotFunctions.histPrePostScale(lightsheet_data, ['count', 'count_norm'], dirDict)\n",
    "# plotFunctions.data_heatmap(lightsheet_data, 'abbreviation', 'count_norm', dirDict)\n",
    "# plotFunctions.data_heatmap_single(lightsheet_data, 'abbreviation', 'cell_density', dirDict)\n",
    "\n",
    "# Statistics\n",
    "# lightsheet_data_filt = helperFunctions.filter_features(lightsheet_data, classifyDict)\n",
    "\n",
    "# classifyDict['dimRed'] = 'SVC'\n",
    "# plotFunctions.dim_red_plot(lightsheet_data_filt, classifyDict, dirDict)\n",
    "\n",
    "# plotFunctions.distance_matrix(lightsheet_data, classifyDict, dirDict)\n",
    "# plotFunctions.correlation_plot(lightsheet_data, classifyDict, dirDict)\n",
    "# plotFunctions.correlation_plot_hier(lightsheet_data, classifyDict, dirDict)\n",
    "\n",
    "# classifyFunctions.classifySamples(lightsheet_data, classifyDict, dirDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifyVec = ['drug', 'class_5HT2A', 'class_KetPsi', 'class_5HTR', 'class_Trypt', 'class_Speed', 'class_Psy_NMDA', 'class_SSRI']\n",
    "classifyVec = ['class_PsiSSRI']\n",
    "classifyDict['model_featureSel'] = 'Boruta' # 'Univar', 'mutInfo', 'RFE', 'MRMR', 'Fdr', 'Fwe_BH', 'Fwe', 'Boruta', 'None'\n",
    "\n",
    "for i in classifyVec:\n",
    "    try:\n",
    "        print(f\"Classifying {i}\")\n",
    "        classifyDict['label'] = i\n",
    "        classifyFunctions.classifySamples(lightsheet_data, classifyDict, dirDict)\n",
    "    except:\n",
    "      print(f\"\\n Failed to classify {i}\")\n",
    "\n",
    "# Rerun everything with 30 best features, as determined by univariate feature selection\n",
    "classifyDict['model_featureSel'] = 'Univar' # 'Univar', 'mutInfo', 'RFE', 'MRMR', 'Fdr', 'Fwe_BH', 'Fwe', 'Boruta', 'None'\n",
    "classifyDict['model_featureSel_k'] = [30]\n",
    "\n",
    "for i in classifyVec:\n",
    "    try:\n",
    "        print(f\"Classifying {i} with Univar\")\n",
    "        classifyDict['label'] = i\n",
    "        classifyFunctions.classifySamples(lightsheet_data, classifyDict, dirDict)\n",
    "    except:\n",
    "      print(f\"\\n Failed to classify {i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cFosV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
