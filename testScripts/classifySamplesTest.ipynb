{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test script for classification\n",
    "# Import packages\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Good idea to add this folder to the json.settings file as \"python.analysis.extraPaths\".\n",
    "sys.path.append('../functionScripts/')\n",
    "\n",
    "import plotFunctions # totalCountsPlot, data_heatmap, correlation_plot\n",
    "import analysisFunctions\n",
    "from initFunctions import createDirs, debugReport, loadLightSheetData\n",
    "import classifyFunctions\n",
    "import helperFunctions\n",
    "\n",
    "# Set Paths to data and output\n",
    "dirDict = dict()\n",
    "rootDir = 'C:\\OneDrive\\MEng\\KwanLab\\Lightsheet_cFos_Pipeline\\\\'\n",
    "dirDict['atlasDir'] = rootDir + 'Atlas\\\\'\n",
    "dirDict['dataDir'] = rootDir + 'Data\\\\'\n",
    "dirDict['B1'] =       dirDict['dataDir'] + 'lightSheetV1\\\\'\n",
    "dirDict['B2'] =       dirDict['dataDir'] + 'lightSheetV2Rev\\\\'   #3/6/23 - Looking at the new, Realigned batch 2 data. #Realigned\n",
    "dirDict['B2_Orig'] =  dirDict['dataDir'] + 'lightSheetV2\\\\'\n",
    "dirDict['B3'] =       dirDict['dataDir'] + 'lightSheetV3\\\\'      #3/6/23 - Batch 3 with MDMA\n",
    "\n",
    "batchSplit = False          # Splits drugs from the first batch of data, from the second, from the 3rd. Batch 1 is labeled with 'a' (aSAL, aKET, aPSI), Batch 3 (cKET, MDMA)\n",
    "splitTag  = ['a', '', 'c']  # Appended the to beginning of data from the first batch (PSI, KET, SAL -> aPSI, KET, aSAL).\n",
    "testSplit = False           # Splits an individual drug for the sake of examining self-similarity\n",
    "oldBatch2 = False\n",
    "\n",
    "debugOutputs = False        # Saves csvs at intervals\n",
    "scalingFactor = True        # Applies 1/total_cells as a scaling factor per mouse.\n",
    "debug_ROI = ['Dorsal nucleus raphe']\n",
    "outputFormat = 'png'\n",
    "\n",
    "switchDir = dict(testSplit=testSplit, batchSplit=batchSplit, splitTag=splitTag, oldBatch2=oldBatch2, debugOutputs=debugOutputs, scalingFactor=scalingFactor, debug_ROI=debug_ROI, outputFormat=outputFormat)\n",
    "\n",
    "# Make directories, and add their strings to the directory dictionary.\n",
    "dirDict = createDirs(rootDir, switchDir, dirDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload in case anything updated in these functions\n",
    "importlib.reload(classifyFunctions)\n",
    "importlib.reload(plotFunctions)\n",
    "importlib.reload(helperFunctions)\n",
    "\n",
    "# Identify parameters of feature clustering, per\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html\n",
    "\n",
    "classifyDict = dict()\n",
    "\n",
    "# Parameters for pivoting the data\n",
    "classifyDict['data'] = 'cell_density' #cell_density, count\n",
    "classifyDict['feature'] = 'abbreviation'\n",
    "classifyDict['label'] = 'drug'\n",
    "\n",
    "# Parameters for feature scaling and aggregation\n",
    "classifyDict['featurefilt'] = False\n",
    "classifyDict['featureAgg'] = False\n",
    "classifyDict['featureSel_scale'] = False # True, False\n",
    "classifyDict['featureSel_linkage'] = 'average'  # 'average', 'complete', 'single', 'ward' (if euclidean)\n",
    "classifyDict['featureSel_distance'] = 'correlation' # 'correlation, 'cosine', 'euclidean'\n",
    "classifyDict['cluster_count'] = 100 # Number of clusters to generate. Not used at the moment.\n",
    "classifyDict['cluster_thres'] = 0.3 # Anything closer than this is merged into a cluster\n",
    "\n",
    "# Parameters for classification\n",
    "classifyDict['model_featureScale'] = True # True, False\n",
    "classifyDict['multiclass'] = 'multinomial' # 'ovr', 'multinomial'\n",
    "\n",
    "classifyDict['model_featureSel'] = 'Univar' # 'Univar', 'mutInfo', 'RFE', 'None'\n",
    "classifyDict['model_featureSel_k'] = [70, 75, 80]\n",
    "# classifyDict['model_featureSel_k'] = [1, 2, 3]\n",
    "\n",
    "# classifyDict['model'] = 'LogRegElastic'\n",
    "classifyDict['model'] = 'LogRegL2'\n",
    "# classifyDict['model'] = 'LogRegL1'\n",
    "\n",
    "# ParamGrid Features - in instances where gridCV is set to true, these are the parameters that will be tested.\n",
    "paramGrid = dict()\n",
    "# paramGrid['classif__l1_ratio'] = [0, 0.1, 0.25, 0.5, 0.75, 0.9, 1]          # used for ElasticNet\n",
    "paramGrid['classif__C'] = [1, 10, 100]       # used for LogisticRegression\n",
    "classifyDict['pGrid'] = paramGrid\n",
    "\n",
    "classifyDict['shuffle'] = False\n",
    "classifyDict['gridCV'] = True\n",
    "classifyDict['CV_count'] = 8 # Number of folds for cross-validation\n",
    "classifyDict['max_iter'] = 200\n",
    "\n",
    "# Load Pickle\n",
    "lightsheet_data = pd.read_pickle('lightsheet_data.pkl')\n",
    "\n",
    "# Big Data - the dataset prior to filtering based on summary structures\n",
    "# lightsheet_data = pd.read_pickle('lightsheet_data_big.pkl')\n",
    "# lightsheet_data.rename(columns={'Brain Area': 'Brain_Area'}, inplace=True)\n",
    "\n",
    "# Pure visualizations\n",
    "# plotFunctions.data_heatmap(lightsheet_data_filt, classifyDict, dirDict)\n",
    "# plotFunctions.data_heatmap_single(lightsheet_data, classifyDict, dirDict)\n",
    "\n",
    "# Statistics\n",
    "# plotFunctions.correlation_plot(lightsheet_data, classifyDict, dirDict)\n",
    "# plotFunctions.distance_matrix(lightsheet_data, classifyDict, dirDict)\n",
    "# plotFunctions.correlation_plot_hier(lightsheet_data, classifyDict, dirDict)\n",
    "\n",
    "classifyFunctions.classifySamples(lightsheet_data, classifyDict, dirDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "dimRedTech = 1\n",
    "# Generate example data\n",
    "# data, labels = make_blobs(n_samples=64, centers=4, n_features=297, random_state=42)\n",
    "\n",
    "if classifyDict['featureSel_filter']:\n",
    "    pandasdf = helperFunctions.filter_and_agg_Data(lightsheet_data, classifyDict)\n",
    "\n",
    "# if classifyDict['featureSel_agg']:\n",
    "if 0:\n",
    "    ls_data_agg = helperFunctions.agg_cluster(pandasdf, classifyDict, dirDict)\n",
    "else:\n",
    "    ls_data_agg = pandasdf.pivot(index='dataset', columns=classifyDict['feature'], values=classifyDict['data'])    \n",
    "\n",
    "ls_data_agg = ls_data_agg.sample(frac=1)\n",
    "\n",
    "# X, y, featureNames, numYDict = hf.reformatData(pandasdf, classifyDict)\n",
    "X = np.array(ls_data_agg.values)\n",
    "# y = np.array([x[0:-1] for x in np.array(ls_data_agg.index)])\n",
    "yStr = np.array([x[0:-1] for x in np.array(ls_data_agg.index)])\n",
    "yDict = dict(zip(np.unique(yStr), range(1, len(np.unique(yStr))+1)))\n",
    "y = np.array([yDict[x] for x in yStr])\n",
    "\n",
    "data = X\n",
    "labels = y\n",
    "dimCount = 3\n",
    "\n",
    "# Perform PCA\n",
    "if dimRedTech == 1:\n",
    "    pca = PCA(n_components=dimCount)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    axesLabel = 'PC '\n",
    "    plotTitle = \"PCA Scatter Plot\"\n",
    "    dimRedTitle = f\"Top{dimCount}_PCA\"\n",
    "elif dimRedTech == 2:\n",
    "    lda = LinearDiscriminantAnalysis(n_components=dimCount)\n",
    "    data_pca = lda.fit_transform(data, labels)\n",
    "    axesLabel = 'LD '\n",
    "    plotTitle = \"LDA Scatter Plot\"\n",
    "    dimRedTitle = f\"Top{dimCount}_LDA\"\n",
    "\n",
    "\n",
    "# Create a scatter plot of the first two PCs, color-coded by labels\n",
    "PCAset = [[1, 2], [2, 3], [1, 3]]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=dimCount,\n",
    "                    figsize=(18, 5))\n",
    "\n",
    "for idx, PCset in enumerate(PCAset):\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = plt.cm.get_cmap('Dark2', len(unique_labels))\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        axes[idx].scatter(data_pca[labels == label, PCset[0]-1], data_pca[labels == label, PCset[1]-1], color=colors(i), label=f\"Label {label}\")\n",
    "\n",
    "    # Add title, legend, and axis labels\n",
    "    axes[idx].set_title(plotTitle)\n",
    "    axes[idx].set_xlabel(f\"{axesLabel}{PCset[0]}\")\n",
    "    axes[idx].set_ylabel(f\"{axesLabel}{PCset[1]}\")\n",
    "    axes[idx].legend(np.unique(yStr))\n",
    "\n",
    "    plt.savefig(dirDict['classifyDir'] + dimRedTitle + '.png', dpi=300, format='png', bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cFosV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
