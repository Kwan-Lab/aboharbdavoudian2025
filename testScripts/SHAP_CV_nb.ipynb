{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing, linear_model\n",
    "\n",
    "sys.path.append('../LSP_Repo/')\n",
    "\n",
    "import classifyFunctions\n",
    "\n",
    "\n",
    "# Load my data\n",
    "# Load Pickle\n",
    "lightsheet_data = pd.read_pickle('lightsheet_data.pkl')\n",
    "\n",
    "classifyDict = dict()\n",
    "classifyDict['data'] = 'count'\n",
    "classifyDict['label'] = 'drug'\n",
    "classifyDict['model'] = 'LogRegL1'\n",
    "classifyDict['model_featureSel'] = 'L1'\n",
    "classifyDict['model_classify'] = 'L2'\n",
    "classifyDict['shuffle'] = True\n",
    "classifyDict['includeSAL'] = True\n",
    "classifyDict['include6FDET'] = True\n",
    "classifyDict['remove_high_corr'] = True\n",
    "classifyDict['corrThreshold'] = 0.8\n",
    "classifyDict['gridCV'] = False\n",
    "\n",
    "_, _, featureNames, numYDict = classifyFunctions.reformatData(lightsheet_data, classifyDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------ CV Repeat number: 0\n",
      "\n",
      "------ Fold Number: 0\n"
     ]
    }
   ],
   "source": [
    "# The standard SHAP procedure \n",
    "import re\n",
    "\n",
    "# Load data\n",
    "url = 'https://raw.githubusercontent.com/Sketchjar/MachineLearningHD/main/boston_data.csv'\n",
    "# df = pd.read_csv(url); df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "lightsheet_data_classify = lightsheet_data.pivot(index='dataset', columns='abbreviation', values='count')\n",
    "lightsheet_data_classify = np.round(lightsheet_data_classify)\n",
    "\n",
    "# Shuffle it to ensure that the sequence isn't informative\n",
    "lightsheet_data_classify = lightsheet_data_classify.sample(frac=1)\n",
    "\n",
    "# Not sure why splits are being called on the pd.df but I'll do it the same.\n",
    "df = lightsheet_data_classify\n",
    "y = pd.Series([re.sub(r'\\d+$', '', string) for string in lightsheet_data_classify.index])\n",
    "X = lightsheet_data_classify.reset_index(drop=True)\n",
    "\n",
    "#Establish CV scheme\n",
    "CV = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "# Libraries for this section \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "ix_training, ix_test = [], []\n",
    "# Loop through each fold and append the training & test indices to the empty lists above\n",
    "for fold in CV.split(df):\n",
    "    ix_training.append(fold[0]), ix_test.append(fold[1])\n",
    "\n",
    "SHAP_values_per_fold = [] #-#-#\n",
    "\n",
    "np.random.seed(1) # Reproducibility \n",
    "CV_repeats = 10\n",
    "# Make a list of random integers between 0 and 10000 of length = CV_repeats to act as different data splits\n",
    "random_states = np.random.randint(10000, size=CV_repeats) \n",
    "\n",
    "######## Use a dict to track the SHAP values of each observation per CV repitition \n",
    "\n",
    "shap_values_per_cv = dict()\n",
    "for sample in X.index:\n",
    "    ## Create keys for each sample\n",
    "    shap_values_per_cv[sample] = {} \n",
    "    ## Then, keys for each CV fold within each sample\n",
    "    for CV_repeat in range(CV_repeats):\n",
    "        shap_values_per_cv[sample][CV_repeat] = {}\n",
    "\n",
    "# Split data, establish model, fit model, make prediction, score model, print result\n",
    "for i, CV_repeat in enumerate(range(CV_repeats)): #-#-#\n",
    "    #Verbose \n",
    "    print('\\n------------ CV Repeat number:', CV_repeat)\n",
    "    #Establish CV scheme\n",
    "    CV = KFold(n_splits=5, shuffle=True, random_state=random_states[i]) # Set random state \n",
    "\n",
    "    ix_training, ix_test = [], []\n",
    "    # Loop through each fold and append the training & test indices to the empty lists above\n",
    "    for fold in CV.split(df):\n",
    "        ix_training.append(fold[0]), ix_test.append(fold[1])\n",
    "        \n",
    "    ## Loop through each outer fold and extract SHAP values \n",
    "    for i, (train_outer_ix, test_outer_ix) in enumerate(zip(ix_training, ix_test)): \n",
    "        #Verbose\n",
    "        print('\\n------ Fold Number:',i)\n",
    "        X_train, X_test = X.iloc[train_outer_ix, :], X.iloc[test_outer_ix, :]\n",
    "        y_train, y_test = y.iloc[train_outer_ix], y.iloc[test_outer_ix]\n",
    "\n",
    "        # model = RandomForestRegressor(random_state=10) # Random state for reproducibility (same results every time)\n",
    "        # fit = model.fit(X_train, y_train)\n",
    "        # yhat = fit.predict(X_test)\n",
    "        # result = mean_squared_error(y_test, yhat)\n",
    "\n",
    "        model = sklearn.linear_model.LogisticRegression(penalty=\"l2\", C=0.1, solver=\"lbfgs\") #multi_class=\"ovr\"\n",
    "        pipeline = make_pipeline(preprocessing.RobustScaler(), model)\n",
    "        pipelineT = make_pipeline(preprocessing.RobustScaler())\n",
    "        pipelineT.fit(X_train, y_train); pipeline.fit(X_train, y_train)\n",
    "        X_train_trans = pipelineT.transform(X_train)\n",
    "\n",
    "        fit = pipeline.fit(X_train_trans, y_train)\n",
    "        yhat = fit.predict(X_test)\n",
    "        # result = mean_squared_error(y_test, yhat)\n",
    "        # print('RMSE:',round(np.sqrt(result),4))\n",
    "\n",
    "        # Use SHAP to explain predictions\n",
    "        # explainer = shap.Explainer(pipeline._final_estimator)\n",
    "        explainer = shap.Explainer(pipeline._final_estimator, X_train_trans, feature_names=X.columns, max_iter=1000)\n",
    "        shap_values = explainer.shap_values(X_train_trans)\n",
    "\n",
    "        # Extract SHAP information per fold per sample \n",
    "        for i, test_index in enumerate(test_outer_ix):\n",
    "            shap_values_per_cv[test_index][CV_repeat] = shap_values[i] #-#-#\n",
    "\n",
    "# Establish lists to keep average Shap values, their Stds, and their min and max\n",
    "average_shap_values, stds, ranges = [],[],[]\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    df_per_obs = pd.DataFrame.from_dict(shap_values_per_cv[i]) # Get all SHAP values for sample number i\n",
    "    # Get relevant statistics for every sample \n",
    "    average_shap_values.append(df_per_obs.mean(axis=1).values) \n",
    "    stds.append(df_per_obs.std(axis=1).values)\n",
    "    ranges.append(df_per_obs.max(axis=1).values-df_per_obs.min(axis=1).values)\n",
    "\n",
    "new_index = [ix for ix_test_fold in ix_test for ix in ix_test_fold]\n",
    "\n",
    "shap.summary_plot(np.array(average_shap_values), X, show = False)\n",
    "plt.title('Average SHAP values after 10x cross-validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard SHAP procedure \n",
    "import re\n",
    "\n",
    "# Load data\n",
    "url = 'https://raw.githubusercontent.com/Sketchjar/MachineLearningHD/main/boston_data.csv'\n",
    "# df = pd.read_csv(url); df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "lightsheet_data_classify = reshaped_data = lightsheet_data.pivot(index='dataset', columns='abbreviation', values='count')\n",
    "y = [re.sub(r'\\d+$', '', string) for string in lightsheet_data_classify.index]\n",
    "X = lightsheet_data_classify.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(new_list)\n",
    "\n",
    "\n",
    "#Establish CV scheme\n",
    "CV = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "# Libraries for this section \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "ix_training, ix_test = [], []\n",
    "# Loop through each fold and append the training & test indices to the empty lists above\n",
    "for fold in CV.split(df):\n",
    "    ix_training.append(fold[0]), ix_test.append(fold[1])\n",
    "\n",
    "SHAP_values_per_fold = [] #-#-#\n",
    "\n",
    "np.random.seed(1) # Reproducibility \n",
    "CV_repeats = 10\n",
    "# Make a list of random integers between 0 and 10000 of length = CV_repeats to act as different data splits\n",
    "random_states = np.random.randint(10000, size=CV_repeats) \n",
    "\n",
    "######## Use a dict to track the SHAP values of each observation per CV repitition \n",
    "\n",
    "shap_values_per_cv = dict()\n",
    "for sample in X.index:\n",
    "    ## Create keys for each sample\n",
    "    shap_values_per_cv[sample] = {} \n",
    "    ## Then, keys for each CV fold within each sample\n",
    "    for CV_repeat in range(CV_repeats):\n",
    "        shap_values_per_cv[sample][CV_repeat] = {}\n",
    "\n",
    "# Split data, establish model, fit model, make prediction, score model, print result\n",
    "for i, CV_repeat in enumerate(range(CV_repeats)): #-#-#\n",
    "    #Verbose \n",
    "    print('\\n------------ CV Repeat number:', CV_repeat)\n",
    "    #Establish CV scheme\n",
    "    CV = KFold(n_splits=5, shuffle=True, random_state=random_states[i]) # Set random state \n",
    "\n",
    "    ix_training, ix_test = [], []\n",
    "    # Loop through each fold and append the training & test indices to the empty lists above\n",
    "    for fold in CV.split(df):\n",
    "        ix_training.append(fold[0]), ix_test.append(fold[1])\n",
    "        \n",
    "    ## Loop through each outer fold and extract SHAP values \n",
    "    for i, (train_outer_ix, test_outer_ix) in enumerate(zip(ix_training, ix_test)): \n",
    "        #Verbose\n",
    "        print('\\n------ Fold Number:',i)\n",
    "        X_train, X_test = X.iloc[train_outer_ix, :], X.iloc[test_outer_ix, :]\n",
    "        y_train, y_test = y.iloc[train_outer_ix], y.iloc[test_outer_ix]\n",
    "\n",
    "        model = RandomForestRegressor(random_state=10) # Random state for reproducibility (same results every time)\n",
    "        fit = model.fit(X_train, y_train)\n",
    "        yhat = fit.predict(X_test)\n",
    "        result = mean_squared_error(y_test, yhat)\n",
    "        print('RMSE:',round(np.sqrt(result),4))\n",
    "\n",
    "        # Use SHAP to explain predictions\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "        # Extract SHAP information per fold per sample \n",
    "        for i, test_index in enumerate(test_outer_ix):\n",
    "            shap_values_per_cv[test_index][CV_repeat] = shap_values[i] #-#-#\n",
    "\n",
    "# Establish lists to keep average Shap values, their Stds, and their min and max\n",
    "average_shap_values, stds, ranges = [],[],[]\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    df_per_obs = pd.DataFrame.from_dict(shap_values_per_cv[i]) # Get all SHAP values for sample number i\n",
    "    # Get relevant statistics for every sample \n",
    "    average_shap_values.append(df_per_obs.mean(axis=1).values) \n",
    "    stds.append(df_per_obs.std(axis=1).values)\n",
    "    ranges.append(df_per_obs.max(axis=1).values-df_per_obs.min(axis=1).values)\n",
    "\n",
    "new_index = [ix for ix_test_fold in ix_test for ix in ix_test_fold]\n",
    "\n",
    "shap.summary_plot(np.array(average_shap_values), X, show = False)\n",
    "plt.title('Average SHAP values after 10x cross-validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard SHAP procedure \n",
    "import re\n",
    "\n",
    "# Load data\n",
    "url = 'https://raw.githubusercontent.com/Sketchjar/MachineLearningHD/main/boston_data.csv'\n",
    "# df = pd.read_csv(url); df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "lightsheet_data_classify = reshaped_data = lightsheet_data.pivot(index='dataset', columns='abbreviation', values='count')\n",
    "y = [re.sub(r'\\d+$', '', string) for string in lightsheet_data_classify.index]\n",
    "X = lightsheet_data_classify.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(new_list)\n",
    "\n",
    "\n",
    "#Establish CV scheme\n",
    "CV = KFold(n_splits=5, shuffle=True, random_state=10)\n",
    "\n",
    "# Libraries for this section \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "ix_training, ix_test = [], []\n",
    "# Loop through each fold and append the training & test indices to the empty lists above\n",
    "for fold in CV.split(df):\n",
    "    ix_training.append(fold[0]), ix_test.append(fold[1])\n",
    "\n",
    "SHAP_values_per_fold = [] #-#-#\n",
    "\n",
    "np.random.seed(1) # Reproducibility \n",
    "CV_repeats = 10\n",
    "# Make a list of random integers between 0 and 10000 of length = CV_repeats to act as different data splits\n",
    "random_states = np.random.randint(10000, size=CV_repeats) \n",
    "\n",
    "######## Use a dict to track the SHAP values of each observation per CV repitition \n",
    "\n",
    "shap_values_per_cv = dict()\n",
    "for sample in X.index:\n",
    "    ## Create keys for each sample\n",
    "    shap_values_per_cv[sample] = {} \n",
    "    ## Then, keys for each CV fold within each sample\n",
    "    for CV_repeat in range(CV_repeats):\n",
    "        shap_values_per_cv[sample][CV_repeat] = {}\n",
    "\n",
    "# Split data, establish model, fit model, make prediction, score model, print result\n",
    "for i, CV_repeat in enumerate(range(CV_repeats)): #-#-#\n",
    "    #Verbose \n",
    "    print('\\n------------ CV Repeat number:', CV_repeat)\n",
    "    #Establish CV scheme\n",
    "    CV = KFold(n_splits=5, shuffle=True, random_state=random_states[i]) # Set random state \n",
    "\n",
    "    ix_training, ix_test = [], []\n",
    "    # Loop through each fold and append the training & test indices to the empty lists above\n",
    "    for fold in CV.split(df):\n",
    "        ix_training.append(fold[0]), ix_test.append(fold[1])\n",
    "        \n",
    "    ## Loop through each outer fold and extract SHAP values \n",
    "    for i, (train_outer_ix, test_outer_ix) in enumerate(zip(ix_training, ix_test)): \n",
    "        #Verbose\n",
    "        print('\\n------ Fold Number:',i)\n",
    "        X_train, X_test = X.iloc[train_outer_ix, :], X.iloc[test_outer_ix, :]\n",
    "        y_train, y_test = y.iloc[train_outer_ix], y.iloc[test_outer_ix]\n",
    "\n",
    "        model = RandomForestRegressor(random_state=10) # Random state for reproducibility (same results every time)\n",
    "        fit = model.fit(X_train, y_train)\n",
    "        yhat = fit.predict(X_test)\n",
    "        result = mean_squared_error(y_test, yhat)\n",
    "        print('RMSE:',round(np.sqrt(result),4))\n",
    "\n",
    "        # Use SHAP to explain predictions\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "        # Extract SHAP information per fold per sample \n",
    "        for i, test_index in enumerate(test_outer_ix):\n",
    "            shap_values_per_cv[test_index][CV_repeat] = shap_values[i] #-#-#\n",
    "\n",
    "# Establish lists to keep average Shap values, their Stds, and their min and max\n",
    "average_shap_values, stds, ranges = [],[],[]\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    df_per_obs = pd.DataFrame.from_dict(shap_values_per_cv[i]) # Get all SHAP values for sample number i\n",
    "    # Get relevant statistics for every sample \n",
    "    average_shap_values.append(df_per_obs.mean(axis=1).values) \n",
    "    stds.append(df_per_obs.std(axis=1).values)\n",
    "    ranges.append(df_per_obs.max(axis=1).values-df_per_obs.min(axis=1).values)\n",
    "\n",
    "new_index = [ix for ix_test_fold in ix_test for ix in ix_test_fold]\n",
    "\n",
    "shap.summary_plot(np.array(average_shap_values), X, show = False)\n",
    "plt.title('Average SHAP values after 10x cross-validation')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cFosV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
